{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c86652-1c2a-4f65-b973-df8299de5154",
   "metadata": {},
   "source": [
    "# Ollama 一键安装启动\n",
    "第一次运行选择 >> \"重启内核并运行所有单元格\"。安装步骤\n",
    "1. 安装魔塔下载器\n",
    "2. 下载 Ollama 安装包\n",
    "3. 安装 Ollama\n",
    "4. 启动 Ollama 服务\n",
    "5. 运行 Ollama\n",
    "\n",
    "https://ollama.com/library 查看支持的模型\n",
    "\n",
    "## 1. 安装 ModelScope 下载器\n",
    "详见 [Ollama-Linux安装](https://www.modelscope.cn/models/modelscope/ollama-linux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05769cda-6a44-406b-87d5-b0d71e8947ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install modelscope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b21551-98c6-40a3-9b4f-2b4458d1c5d5",
   "metadata": {},
   "source": [
    "## 2. 从 ModelScope 上下载 Ollama 安装包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d521c88-4868-41a2-baa0-1f3d2e6fd2dd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!modelscope download --model=modelscope/ollama-linux --local_dir /root/gpufree-data/ollama-linux --revision v0.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef994d39-055c-4f48-b792-d5b170ddbf6d",
   "metadata": {},
   "source": [
    "## 3. 安装 ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a88362c-5ba3-45ad-a35f-18893faa3d7b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Installing Ollama to /usr/local/bin\n",
      ">>> Unzipping Ollama-bundle to /usr/local...\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      ">>> systemd is not running\n",
      ">>> NVIDIA GPU installed.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p /root/gpufree-data/ollama-data && ln -s /root/gpufree-data/ollama-data /root/.ollama\n",
    "!cd /root/gpufree-data/ollama-linux && chmod 777 ./ollama-modelscope-install.sh && ./ollama-modelscope-install.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e59ce1f-4ba5-4d5b-9522-14ac234536c2",
   "metadata": {},
   "source": [
    "## 4. 启动 Ollama 服务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71917fcd-699b-4f8d-8062-4ab243a933a5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!ollama serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785df34-340d-49a8-8136-5a126b4431c9",
   "metadata": {},
   "source": [
    "## 5. 运行 Ollama\n",
    "在终端输入 `ollama run llama3.2`\n",
    "\n",
    "或者在控制台，选择 **自定义服务** 打开 11434 端口，用 Chatbox 等客户端连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b2078-c736-4b05-8148-7f768b08943e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
